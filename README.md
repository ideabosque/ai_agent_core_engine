### üåç Stateless. Scalable. Multi-LLM.

# **The Next Evolution in AI Agent Orchestration (AI Agent Core Engine)**

Run and scale intelligent agents across **multiple LLMs** ‚Äî without managing state or sessions.
Built on the **SilvaEngine serverless framework**, our platform delivers **rolling context memory**, modular **function calling**, and real-time **conversation monitoring** ‚Äî all in one unified, AI-native control plane.

**‚ö° Stateless by design** ‚Äì infinite scale, instant recovery

**üß† Model-agnostic** ‚Äì OpenAI, Anthropic, Gemini, and more

**üîå Modular architecture** ‚Äì plug-and-play function routing

**üìà Fully observable** ‚Äì every conversation, recorded and traceable

## **Introduction**

üåê **The World‚Äôs First Stateless Multi-LLM Agent Orchestration Platform**

Welcome to the future of AI agent infrastructure ‚Äî a revolutionary platform that redefines how intelligent agents are deployed, scaled, and orchestrated across diverse LLM ecosystems. Powered by the **SilvaEngine serverless framework**, this stateless platform offers unmatched agility, reliability, and speed in handling AI-driven conversations at scale.

üîÅ **Stateless by Design. Context-Rich by Architecture.**
Say goodbye to complex session management. Our platform enables **rolling context memory**, ensuring every conversation retains critical relevance ‚Äî without the weight of persistent storage. This lets you scale infinitely, recover instantly, and run lightweight AI agents that remain deeply aware and responsive.

üß† **Multi-Model Intelligence. Modular Functionality.**
Orchestrate and switch between top-tier LLMs ‚Äî OpenAI, Anthropic, Gemini, and more ‚Äî in real time. Our modular function-calling engine enables **plug-and-play capabilities**, allowing agents to invoke domain-specific tools and workflows, no matter which model powers the response.

üîç **Thread-Aware Monitoring & Observability**
Every conversation is **recorded, versioned, and monitored**, giving your teams full visibility into interaction history, decision logic, and user intent. Gain insights, audit compliance, and optimize performance with intelligent observability built into the core.

üß© **Built for Builders. Trusted by Enterprises.**
Whether you're launching autonomous agents, enhancing customer service, or powering next-gen copilots, this platform equips you with the **flexibility of serverless**, the **power of orchestration**, and the **precision of stateless memory** ‚Äî all in one unified, AI-native control plane.

üîê **Secure. Scalable. Future-Proof.**
Designed to meet the needs of mission-critical applications, our platform embraces security-first principles, auto-scaling infrastructure, and modular integration to support evolving enterprise demands in AI deployment.

### Key Features

#### üîß **Core Features**

| **Feature**                            | **Description**                                                              |
| -------------------------------------- | ---------------------------------------------------------------------------- |
| **Stateless Architecture**             | Eliminates session handling for infinite scalability and instant recovery.   |
| **Rolling Context Memory**             | Maintains conversational context without persisting full session state.      |
| **Multi-LLM Orchestration**            | Real-time switching and integration across OpenAI, Anthropic, Gemini, etc.   |
| **Modular Function Calling**           | Plug-and-play routing of tool/function calls across different models.        |
| **Serverless Framework (SilvaEngine)** | Built on a scalable, lightweight, cloud-native infrastructure.               |
| **Model-Agnostic Compatibility**       | Supports diverse LLM providers with seamless fallback or parallel execution. |

---

#### üß† **Intelligence & Functionality**

| **Feature**                              | **Description**                                                                |
| ---------------------------------------- | ------------------------------------------------------------------------------ |
| **Thread-Aware Conversation Monitoring** | Tracks and logs conversations with full visibility and lineage.                |
| **Real-Time Decision Traceability**      | Each agent interaction is versioned and auditable for debugging or compliance. |
| **Autonomous Agent Enablement**          | Designed to support agent autonomy with dynamic decision-making.               |
| **Domain-Specific Tool Invocation**      | Agents can call tools dynamically based on domain and user context.            |

---

#### üìä **Observability & Control**

| **Feature**                      | **Description**                                                     |
| -------------------------------- | ------------------------------------------------------------------- |
| **Full Interaction Logging**     | Stores every user-agent exchange for analytics and optimization.    |
| **Versioned Execution Contexts** | Enables replay and regression testing of agent decisions.           |
| **Live Conversation Monitoring** | Allows real-time viewing and intervention of ongoing agent threads. |

---

#### üîê **Enterprise-Grade Infrastructure**

| **Feature**                    | **Description**                                                            |
| ------------------------------ | -------------------------------------------------------------------------- |
| **Security-First Design**      | Includes audit trails, identity boundaries, and secure execution policies. |
| **Auto-Scaling**               | Grows with usage demand without manual provisioning.                       |
| **Modular Integration Layer**  | Easily connects to internal tools, APIs, or databases.                     |
| **Future-Proof Compatibility** | Adapts to evolving AI models and enterprise infrastructure needs.          |


### üß† **Stateless Multi-LLM AI Agent Core Engine ‚Äî Architecture Overview**
![AI Agent Core Engine Architecture Diagram](/images/ai_agent_core_engine_architecture.jpg)

This diagram showcases a **serverless, multi-LLM AI agent orchestration system** powered by **SilvaEngine**. It supports **real-time interactions over WebSocket**, integrates **multiple LLMs (OpenAI, Gemini, Claude)**, and executes tasks using **asynchronous Lambda functions and modular handlers**.

---

#### üîÑ **End-to-End Flow Description (Updated)**

##### 1. **User Interaction (WebSocket Query)**

* The **User** initiates a query via **WebSocket (WSS)**.
* The query is routed through **Amazon API Gateway**, acting as a real-time interface for bidirectional communication.

##### 2. **Initial Message Routing**

* API Gateway forwards the request to an **AWS Lambda** (`SilvaEngine Area Resource`) for validation, routing logic, and enqueueing.
* The message is pushed to **Amazon SQS** (`SilvaEngineTask Queue`) for asynchronous, decoupled task execution.

##### 3. **Agent Task Execution (Async)**

* **SilvaEngine Agent Task**, another Lambda function, dequeues the message and invokes:

  * Tool calling logic.
  * AI agent orchestration.
  * External function integrations.

##### 4. **AI Agent Core Orchestration**

* The **AI Agent Core Engine** acts as the **stateless orchestrator**. Responsibilities include:

  * Managing **conversation context** using **Amazon DynamoDB**.
  * Delegating the query to the appropriate **LLM Agent Handler** based on routing rules or model availability.

##### 5. **Multi-LLM Routing via Agent Handlers**

* The platform supports multiple language models via **dedicated handlers**:

  * **OpenAI Agent Handler** ‚Üí routes to **OpenAI Response API**
  * **Gemini Agent Handler** ‚Üí routes to **Google Gemini API**
  * **Anthropic Agent Handler** ‚Üí routes to **Anthropic Claude API**

Each handler formats, sends, and processes responses independently, enabling **model-agnostic orchestration**.

##### 6. **Final AI Response Handling**

* The **AI Agent Handler** (green box) handles:

  * Response post-processing.
  * Tool call updates.
  * State updates and results formatting.
  * Sends **WebSocket replies** to the user via the original API Gateway connection.

---

#### üåê **New Capabilities Highlighted**

| Feature                    | Description                                                                         |
| -------------------------- | ----------------------------------------------------------------------------------- |
| **True Multi-LLM Support** | Unified engine dynamically selects and communicates with OpenAI, Gemini, or Claude. |
| **Modular Handler Layer**  | Handlers for each model can be independently managed, scaled, or extended.          |
| **Extensible Backend**     | Easily add future models or vendors by plugging in new handler modules.             |

---

### üîÑ **AI Agent Orchestration: Sequence Flow Description**
![AI Agent Core Engine Sequence Diagram](/images/ai_agent_core_engine_sequence_diagram.jpg)

#### üßç‚Äç‚ôÇÔ∏è **1. User Initiates Query**

* The **user** sends a query via WebSocket.
* The message is received by the system and triggers the **Resolve Ask Model** step.

---

#### üîÅ **2. Model Resolution & Execution Initiation**

| Step | Component                   | Description                                                                     |
| ---- | --------------------------- | ------------------------------------------------------------------------------- |
| ‚úÖ    | **Resolve Ask Model**       | Identifies which AI model (e.g., OpenAI, Gemini, Claude) to use.                |
| üîÑ   | **Async Execute Ask Model** | Asynchronously invokes execution logic.                                         |
| üöÄ   | **Execute Ask Model**       | Begins actual agent processing and prepares the message for the selected model. |

---

#### üß† **3. Language Model Interaction**

| Step | Component               | Description                                                                                         |
| ---- | ----------------------- | --------------------------------------------------------------------------------------------------- |
| üì©   | **OpenAIEventHandler**  | Handles communication with the OpenAI API.                                                          |
| ü§ñ   | **OpenAI Response API** | Processes the query and returns a structured response, potentially with function call instructions. |

---

#### üîß **4. Function Calling Workflow (if applicable)**

* If the response includes a **tool/function call**, it‚Äôs passed to the **Function Calling Module**.
* This module:

  * Executes the requested tool/module logic.
  * Triggers **multiple async updates** to the system about the tool's status:

    * üî∏ *Initial*
    * üî∏ *In Progress*
    * üî∏ *Completed*

---

#### üîÑ **5. Tool Call Handling and Agent Coordination**

| Component                         | Description                                                                                        |
| --------------------------------- | -------------------------------------------------------------------------------------------------- |
| **AI Agent Handler**              | Orchestrates the tool call update flow and prepares the final response.                            |
| **AWS SQS**                       | Used for decoupled communication and state update queuing.                                         |
| **Async Insert Update Tool Call** | Lambda functions or microservices that handle stepwise updates (initial ‚Üí in progress ‚Üí complete). |

---

#### üì§ **6. Final Delivery via WebSocket**

* Once the task is completed and results are ready:

  * The system asynchronously triggers **Send Data to WebSocket**.
  * The **user receives the final response** through the established WSS connection.

---

### üß© **Key Flow Highlights**

| Area                        | Purpose                                                                        |
| --------------------------- | ------------------------------------------------------------------------------ |
| ‚úÖ Model Decoupling          | The LLM model execution is abstracted away from the user-facing logic.         |
| üîÑ Asynchronous Operations  | All tool updates and executions are handled via async invokes for scalability. |
| üîå Modular Function Calling | Allows LLM responses to dynamically trigger domain-specific operations.        |
| üì° Real-time Delivery       | Results are delivered back to the user over the original WebSocket channel.    |

---

### üß© **ER Diagram Overview: Modular AI Agent Orchestration System**
![AI Agent Core Engine ER Diagram](/images/ai_agent_core_engine_er_diagram.jpg)

This ER diagram structures the system into the following core **logical domains**:

---

#### üîµ **1. Language Models & Agent Definitions**

##### **`llms`**

* Stores metadata about each supported language model (OpenAI, Anthropic, Gemini, etc.).
* Keys: `llm_provider`, `llm_name`
* Includes: `module_name`, `class_name`, `updated_by`, `created_at`

##### **`agents`**

* Defines each AI agent version and its configuration.
* Keys: `endpoint_id`, `agent_version_uuid`, `agent_uuid`
* Maps to: `llm_provider`, `llm_name`
* Includes function mappings, tool call behavior, and message limits.

---

#### üß† **2. Thread & Message Management**

##### **`threads`**

* Represents a full conversation session (thread) for a user-agent pair.
* Keys: `endpoint_id`, `thread_uuid`
* Associates with: `agent_uuid`, `user_id`

##### **`messages`**

* Stores each individual message in a thread.
* Keys: `thread_uuid`, `message_uuid`, `run_uuid`
* Includes: `role`, `message`, `created_at`, etc.

##### **`runs`**

* Represents a single inference call in a conversation (mapped to a model request).
* Keys: `thread_uuid`, `run_uuid`
* Tracks: token usage, duration, endpoint\_id, etc.

---

#### ‚öôÔ∏è **3. Tool Calling & Async Execution**

##### **`tool_calls`**

* Tracks all function/tool calls invoked by the agent within a thread and run.
* Keys: `thread_uuid`, `tool_call_uuid`, `run_uuid`
* Attributes: `tool_type`, `arguments`, `content`, `status`, `notes`, `time_spent`

##### **`async_tasks`**

* Logs background async operations such as tool executions or external API calls.
* Keys: `function_name`, `async_task_uuid`
* Includes: `endpoint_id`, `arguments`, `result`, `status`, `notes`, `time_spent`

---

#### üß™ **4. Fine-Tuning Data Management**

##### **`fine_tuning_messages`**

* Stores structured messages and tool calls for supervised fine-tuning.
* Keys: `agent_uuid`, `message_uuid`, `thread_uuid`, `timestamp`
* Attributes: `role`, `tool_calls`, `weight`, `trained`

---

#### üîÑ **5. Modular Function Configuration**

##### **`functions`**

* Registry of callable functions used in tool calling.
* Key: `function_name`
* Includes: `function` object with `module_name`, `class_name`, and `configuration`.

##### **`configuration (OpenAI) as an example`**

* A nested structure defining configuration options specific to OpenAI integration as an example.
* Includes: `openai_api_key`, `tools`, `max_output_tokens`, `temperature`, etc.

---

### üîó **Entity Relationships Summary**

| Relationship                                | Description                                                   |
| ------------------------------------------- | ------------------------------------------------------------- |
| `agents ‚Üî llms`                             | Each agent maps to a specific LLM definition.                 |
| `threads ‚Üî agents`                          | A thread is linked to the agent version and endpoint.         |
| `messages ‚Üî threads`                        | Messages are grouped by thread and run.                       |
| `runs ‚Üî threads`                            | Each model inference (run) occurs in a thread context.        |
| `tool_calls ‚Üî runs/messages`                | Tool calls are tied to a specific run and message.            |
| `async_tasks ‚Üî tool_calls`                  | Background tasks are logged independently and asynchronously. |
| `fine_tuning_messages ‚Üî threads/tool_calls` | Enables training data extraction per thread.                  |

---

Certainly! Here's a rephrased and enhanced version:

---

## ‚öôÔ∏è**Deployment and Setup**

To successfully deploy and configure the AI Agent, please follow the detailed instructions provided in the [AI Agent Deployment Guide](https://github.com/ideabosque/ai_agent_deployment). This resource includes step-by-step guidance to ensure a smooth and efficient setup process.

## ü§ñ **Agent Definition & Configuration (Event Handler Layer)**
![AI Agent Event Handler Class Diagram](/images/ai_agent_event_handler_class_diagram.jpg)

This section defines the architecture for how agents are implemented, extended, and executed using a modular, class-based event handling system. It enables **runtime polymorphism** across different language model providers such as OpenAI, Gemini, Anthropic, and Ollama.

---

### üß© **1. Base Class: `AIAgentEventHandler`**

The [`AIAgentEventHandler`](https://github.com/ideabosque/ai_agent_handler) serves as the **abstract base class** for all model-specific agent handlers. It defines the common interface and shared utilities required to run an agent against a target LLM.

#### **Core Attributes**

* `endpoint_id`: Identifier for the active agent.
* `agent_name`, `agent_description`: Metadata for logging and auditing.
* `short_term_memory`: Runtime memory or summarization store.
* `settings_dict`: Loaded model configuration.
* `accumulated_json`: Structured context/data accumulated across turns.

#### **Key Methods**

* `invoke_async_func(...)`: Dynamically invokes a registered Python function.
* `send_data_to_stream(...)`: Streams output back to the user in real time.
* `get_function(...)`: Retrieve and load the target function to enable dynamic invocation during runtime, ensuring it's prepared and ready for execution as part of the function-calling workflow.
* `accumulate_partial_json(...)`: Handlers for structured data processing.

---

### üß† **2. Model-Specific Agent Handlers**

Each subclass provides a concrete implementation of `invoke_model()` for the designated provider.

---

#### ‚úÖ **`OpenAIEventHandler`**

* **Client:** `openai.OpenAI`
* **Attributes:** `model_settings`
* **Method: `invoke_model(...)`**

  * Supports token tracking, function calling (`tool_calls`), and streaming.
  * Invokes OpenAI's Chat API using system/user messages.

Follow the detailed instructions for configration provided in the [OpenAI Agent Handler](https://github.com/ideabosque/openai_agent_handler)

---

#### üåê **`GeminiEventHandler`**

* **Client:** `genai.Client`
* **Attributes:** `model_settings`, `assistant_messages`
* **Method: `invoke_model(...)`**

  * Executes Gemini chat model with `prompt`, `tools`, and event streaming support.

Follow the detailed instructions for configration provided in the [Gemini Agent Handler](https://github.com/ideabosque/gemini_agent_handler)

---

#### üß¨ **`AnthropicEventHandler`**

* **Client:** `anthropic.Anthropic`
* **Attributes:** `model_settings`, `assistant_messages`
* **Method: `invoke_model(...)`**

  * Interacts with Claude via streaming or synchronous completions.
  * Handles threading and function return parsing.

Follow the detailed instructions for configration provided in the [Anthropic Agent Handler](https://github.com/ideabosque/anthropic_agent_handler)

---

#### üß™ **`OllamaEventHandler`**

* **Client:** Embedded/local runtime (no external API)
* **Attributes:** `system_message`, `model_settings`, `tools`
* **Method: `invoke_model(...)`**

  * Integrates with locally hosted models via Ollama (e.g., LLaMA, Mistral).
  * Tool call handling supported through `tool_call`.

Follow the detailed instructions for configration provided in the [Ollama Agent Handler](https://github.com/ideabosque/ollama_agent_handler)

---

## üîÑ **Key Design Benefits**

| Feature                    | Description                                             |
| -------------------------- | ------------------------------------------------------- |
| **Pluggable Architecture** | Add support for any new LLM by implementing a subclass. |
| **Unified Runtime API**    | Standardized agent behavior across models.              |
| **Streaming & Async**      | Natively supports event streaming and async updates.    |
| **Tool Calling**           | Fully integrated function call support across models.   |

---

## üß™ Testing and Prototype

This script provides a unified test harness for validating AI agent orchestration in both:

1. **Local Function Invocation Mode** (`test_run_chatbot_loop_local`)
   - Runs against local GraphQL endpoint and lambda-mimicking functions.
   - Validates internal integration between `askModel`, `asyncTask`, and the core orchestration engine.
   - Useful for debugging logic, schema mapping, and tool execution in development environments.

2. **External Request Mode** (`test_run_chatbot_loop_by_request`)
   - Interacts with deployed GraphQL API via HTTP requests.
   - Emulates real user interactions through RESTful communication.
   - Ideal for testing deployment correctness and system-wide flow.

üîß **Environment Variables Required**:
- `base_dir`, `agent_uuid`, `user_id`
- AWS credentials: `region_name`, `aws_access_key_id`, `aws_secret_access_key`
- API test setup: `api_url`, `api_key`, `endpoint_id`

üß© **Key Integrations**:
- SilvaEngine GraphQL schema loader (`Utility.fetch_graphql_schema`)
- AIAgentCoreEngine task dispatcher and resolver
- Support for multiple LLM backends (OpenAI, Gemini, Anthropic, Ollama) via handler system

```python
#!/usr/bin/python
# -*- coding: utf-8 -*-

import os
import sys
import requests
import unittest
import logging
from dotenv import load_dotenv

load_dotenv()

sys.path.insert(0, f"{os.getenv('base_dir')}/silvaengine_utility")
sys.path.insert(1, f"{os.getenv('base_dir')}/silvaengine_dynamodb_base")
sys.path.insert(2, f"{os.getenv('base_dir')}/ai_agent_core_engine")
sys.path.insert(3, f"{os.getenv('base_dir')}/ai_agent_handler")
sys.path.insert(4, f"{os.getenv('base_dir')}/openai_agent_handler")
sys.path.insert(5, f"{os.getenv('base_dir')}/gemini_agent_handler")
sys.path.insert(6, f"{os.getenv('base_dir')}/anthropic_agent_handler")
sys.path.insert(7, f"{os.getenv('base_dir')}/ollama_agent_handler")

from ai_agent_core_engine import AIAgentCoreEngine
from silvaengine_utility import Utility

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger()

setting = {
    "region_name": os.getenv("region_name"),
    "aws_access_key_id": os.getenv("aws_access_key_id"),
    "aws_secret_access_key": os.getenv("aws_secret_access_key"),
    "endpoint_id": os.getenv("endpoint_id"),
    "connection_id": os.getenv("connection_id"),
    "execute_mode": os.getenv("execute_mode"),
}

class GenericChatbotTest(unittest.TestCase):
    def setUp(self):
        self.ai_agent_core_engine = AIAgentCoreEngine(logger, **setting)
        self.endpoint_id = setting.get("endpoint_id")
        self.schema = Utility.fetch_graphql_schema(
            logger,
            self.endpoint_id,
            "ai_agent_core_graphql",
            setting=setting,
            execute_mode="local_for_all",
        )

    def test_run_chatbot_loop_local(self):
        logger.info("Starting chatbot (local loop mode)...")
        thread_uuid = None

        while True:
            user_input = input("User: ")
            if user_input.strip().lower() in ["exit", "quit"]:
                print("Chatbot: Goodbye!")
                break

            ask_query = Utility.generate_graphql_operation("askModel", "Query", self.schema)
            ask_payload = {
                "query": ask_query,
                "variables": {
                    "agentUuid": os.getenv("agent_uuid"),
                    "threadUuid": thread_uuid,
                    "userQuery": user_input,
                    "userId": os.getenv("user_id"),
                    "stream": False,
                    "updatedBy": "test_user",
                },
            }
            ask_response = Utility.json_loads(self.ai_agent_core_engine.ai_agent_core_graphql(**ask_payload))
            thread_uuid = ask_response["data"]["askModel"]["threadUuid"]

            task_query = Utility.generate_graphql_operation("asyncTask", "Query", self.schema)
            task_payload = {
                "query": task_query,
                "variables": {
                    "functionName": "async_execute_ask_model",
                    "asyncTaskUuid": ask_response["data"]["askModel"]["asyncTaskUuid"],
                },
            }
            task_response = Utility.json_loads(self.ai_agent_core_engine.ai_agent_core_graphql(**task_payload))
            print("Chatbot:", task_response["data"]["asyncTask"]["result"])

    def test_run_chatbot_loop_by_request(self):
        logger.info("Starting chatbot (external request mode)...")

        url = os.getenv("api_url")
        headers = {
            "x-api-key": os.getenv("api_key"),
            "Content-Type": "application/json",
        }

        ask_query = """query askModel($agentUuid: String!, $threadUuid: String, $userQuery: String!, $stream: Boolean, $updatedBy: String!) {
            askModel(agentUuid: $agentUuid, threadUuid: $threadUuid, userQuery: $userQuery, stream: $stream, updatedBy: $updatedBy) {
                agentUuid threadUuid userQuery functionName asyncTaskUuid currentRunUuid
            }
        }"""

        task_query = """query asyncTask($functionName: String!, $asyncTaskUuid: String!) {
            asyncTask(functionName: $functionName, asyncTaskUuid: $asyncTaskUuid) {
                result status
            }
        }"""

        thread_uuid = None
        while True:
            user_input = input("User: ")
            if user_input.strip().lower() in ["exit", "quit"]:
                print("Chatbot: Goodbye!")
                break

            ask_payload = {
                "query": ask_query,
                "variables": {
                    "agentUuid": os.getenv("agent_uuid"),
                    "threadUuid": thread_uuid,
                    "userQuery": user_input,
                    "stream": False,
                    "updatedBy": "test_user",
                },
            }
            ask_response = requests.post(url, json=ask_payload, headers=headers).json()
            thread_uuid = ask_response["data"]["askModel"]["threadUuid"]

            task_payload = {
                "query": task_query,
                "variables": {
                    "functionName": "async_execute_ask_model",
                    "asyncTaskUuid": ask_response["data"]["askModel"]["asyncTaskUuid"],
                },
            }
            while True:
                task_response = requests.post(url, json=task_payload, headers=headers).json()
                if task_response["data"]["asyncTask"]["status"] in ["completed", "failed"]:
                    break

            print("Chatbot:", task_response["data"]["asyncTask"]["result"])


if __name__ == '__main__':
    unittest.main()
```

---